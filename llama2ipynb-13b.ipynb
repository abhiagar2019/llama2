{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (4.31.0)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (1.25.0)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.3.2)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1691919775512
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: accelerate in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (1.25.0)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (23.0)\nRequirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: pyyaml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (2.0.1)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\nRequirement already satisfied: setuptools in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (67.8.0)\nRequirement already satisfied: wheel in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\nRequirement already satisfied: cmake in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\nRequirement already satisfied: lit in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691919778392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: huggingface_hub in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub) (23.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface_hub) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691919780561
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub._login import _login\n",
        "_login(token='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', add_to_git_credential=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Token is valid (permission: read).\nYour token has been saved to /home/azureuser/.cache/huggingface/token\nLogin successful\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691919780889
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-13b-chat-hf\" #\"meta-llama/Llama-2-7b-chat-hf\"\n"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691917184116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "tokenizer\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691917184361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float32,#torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "sequences = pipeline(\n",
        "    'Convert the following R code GLM model into Python PySpark script to train XGBoost\\\n",
        "    final_model = glm(Label ~Feature1+Feature2+Feature3, family=binomial, data=train\\n',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading checkpoint shards: 100%|██████████| 3/3 [00:44<00:00, 14.86s/it]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Result: Convert the following R code GLM model into Python PySpark script to train XGBoost    final_model = glm(Label ~Feature1+Feature2+Feature3, family=binomial, data=train\n\nimport pandas as pd\nfrom pyspark.sql.functions import col, glm\nfrom pyspark.sql.types import IntegerType, StringType\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import XGBClassifier\n\n# Load the data\ntrain = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n\n# Convert the data to a PySpark DataFrame\ntrain_df = train.toDF()\n\n# Split the data into training and testing sets\ntrain_df, test_df = train_df.randomSplit([0.7, 0.3], seed=42)\n\n# Create a VectorAssembler to combine the features\nassembler = VectorAssembler(inputCols=[\"Feature1\", \"Feature2\", \"Feature3\"], outputCol=\"features\")\n\n# Fit and transform the data\ntrain_df = assembler.fit(train_df).transform(train_df)\n\n# Train an XGBoost model on the training data\nmodel = XGBClassifier(featuresCol=\"features\", labelCol=\"Label\", maxDepth=6, numClass=8)\nmodel = model.fit(train_df)\n\n# Make predictions on the testing data\npredictions = model.transform(test_df)\n\n# Evaluate the model using log loss\nlog_loss = predictions.select(\"predictions\", \"Label\").groupBy(\"Label\").agg(loss=mean(\"predictions\").sub(mean(\"Label\"))).orderBy(\"loss\").first()[0][\"loss\"]\n\n# Print the log loss\nprint(\"Log loss:\", log_loss)\n\nNote that the R code and the Python PySpark script are doing the same thing - they are both training an XGBoost model on the training data using the GLM family and the specified features. The main difference is in the way the data is processed and the model is trained.\n\nIn R, the data is processed using the built-in `glm()` function, which takes care of the feature engineering and model training. In Python, we need to use the `VectorAssembler` to combine the features and then train the model using the `XGBClassifier` class.\n\nAdditionally, in R, the model is trained using the `family` argument, which specifies the distribution of the response variable. In Python, we need to specify the distribution of the response variable when creating the `XGBClassifier` object.\n\nOverall, both the R code and the Python PySpark script should produce similar results, but the Python script may be more flexible and scalable for large datasets.\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691918239655
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "    'I have productType, productSubType and Sales columns in a SQL table. \\\n",
        "    Write a SQL query to find out what proportion of sales each productSubType contributed \\\n",
        "    within each productType category',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Result: I have productType, productSubType and Sales columns in a SQL table.     Write a SQL query to find out what proportion of sales each productSubType contributed     within each productType category.\n\nFor example, if the table is like this:\n\n| productType | productSubType | Sales |\n|জেএসএস | A            | 100 |\n|জেএসএস | B            | 50 |\n|জেএসএস | C            | 200 |\n| others     | A            | 200 |\n| others     | B            | 150 |\n| others     | C            | 50 |\n\nThe output should be like this:\n\n| productType | productSubType | Sales | Proportion |\n|জেএসএস | A            | 100 | 0.25 |\n|জেএসএস | B            | 50 | 0.17 |\n|জেএসএস | C            | 200 | 0.40 |\n| others     | A            | 200 | 0.50 |\n| others     | B            | 150 | 0.33 |\n| others     | C            | 50 | 0.17 |\n\nNote that the \"Proportion\" column is calculated by dividing the Sales of each productSubType by the total Sales of all productSubTypes within the same productType category, and then multiplying by 100.\n\nPlease help me with this query.\n\nThank you in advance.\n\nBest regards,\n\nS.R.\n\nAnswer: You can use a `SELECT` statement with a `JOIN` and `GROUP BY` to achieve this. Here's the query:\n\n\\begin{code}\nSELECT p.productType, ps.productSubType, SUM(s.Sales) AS Sales,\n       SUM(s.Sales) OVER (PARTITION BY p.productType) /\n       SUM(s.Sales)essional OVER (PARTITION BY p.productType) * 100 AS Proportion\nFROM your_table AS s\nJOIN your_table AS ps ON s.productType = ps.productType\nJOIN your_table AS p ON ps.productType = p.productType\nGROUP BY p.productType, ps.productSubType;\n\\end{code}\n\nHere's how the query works:\n\n1. We first join the table with itself three times to get all possible combinations of `productType` and `productSubType`.\n2. We then group the results by `productType` and `productSubType`.\n3. We use a window function (specifically, `SUM(s.Sales) OVER (PARTITION BY p.productType)`) to calculate the total sales for each `productType`.\n4. We use another window function (specifically, `SUM(s.Sales) OVER (PARTITION BY p.productType) / SUM(s.Sales) OVER (PARTITION BY p.productType)`) to calculate the proportion of sales for each `productSubType` within each RogerType category.\n김chi  We multiply the result by 100 to convert it to a percentage.\n\nNote that you'll need to replace `your_table` with the actual name of your table. Also, make sure you have a `productType` column in your table, as the query assumes that it exists.\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691919169059
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "    'Explain how attention works in LLM',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=2000,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Result: Explain how attention works in LLMs and how it can be used to improve performance.\n\nAttention is a technique used in large language models (LLMs) to allow the model to focus on specific parts of the input when generating output. This is particularly useful when dealing with long sequences of text, as it allows the model to selectively attend to the most relevant parts of the input rather than using a fixed context or considering the entire input equally.\n\nIn LLMs, attention works by computing a weighted sum of the input elements, where the weights are learned during training and reflect the importance of each input element for the specific task at hand. The attention mechanism is typically applied to the input embeddings, which are vector representations of the input words or tokens. The attention weights are then used to compute a weighted sum of the input embeddings, which forms the input to the next layer of the model.\n\nThere are different types of attention mechanisms used in LLMs, including:\n\n1. Scaled Dot-Product Attention: This is a common type of attention mechanism that computes the attention weights by taking the dot product of the query and key vectors, scaling the result by a scalar value, and applying a softmax function to normalize the weights.\n2. Multi-Head Attention: This is an extension of the scaled dot-product attention mechanism that allows the model to jointly attend to information from different representation subspaces at different positions.\n3. Self-Attention: This is a type of attention mechanism that allows the model to attend to different parts of the input sequence simultaneously, which can be useful for tasks such as language translation.\n\nAttention can be used to improve performance in LLMs in several ways:\n\n1. Selective attention: By selectively attending to specific parts of the input, the model can focus on the most relevant information and ignore irrelevant information, which can improve the accuracy and efficiency of the model.\n2. Contextual attention: By attending to the context in which a word or token appears, the model can better understand the meaning and intent of the input, which can improve the accuracy and fluency of the output.\n3. Long-range dependence: By attending to distant parts of the input sequence, the model can capture long-range dependencies and contextual information, which can improve the accuracy and coherence of the output.\n4. Efficient computation: By selectively attending to specific parts of the input, the model can reduce the computational cost of the attention mechanism, which can improve the efficiency of the model.\n\nIn summary, attention is a powerful technique used in LLMashington Postllars to allow the model to selectively focus on specific parts of the input when generating output. It can be applied in different forms, such as scaled dot-product attention, multi-head attention, and self-attention, and can be used to improve performance in various ways, including selective attention, contextual attention, long-range dependence, and efficient computation.\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691919772621
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore down"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691881280604
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-13b-chat-hf\" #\"meta-llama/Llama-2-70b-chat-hf\" #\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691881327380
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691881330147
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name,use_auth_token=True, device_map = 'auto') #.to(\"cuda\") #, "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.39s/it]\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Need either a `state_dict` or a `save_folder` containing offloaded weights.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#.to(\"cuda\") #, \u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    492\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/modeling_utils.py:2959\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(dispatch_model)\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m   2958\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_skip_keys_device_placement\n\u001b[0;32m-> 2959\u001b[0m     \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   2962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/big_modeling.py:364\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m save_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     device \u001b[38;5;241m=\u001b[39m main_device \u001b[38;5;28;01mif\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     weights_map \u001b[38;5;241m=\u001b[39m \u001b[43mOffloadedWeightsLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     weights_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/accelerate/utils/offload.py:150\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__init__\u001b[0;34m(self, state_dict, save_folder, index, device)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    144\u001b[0m     state_dict: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m ):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m save_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed either a `state_dict` or a `save_folder` containing offloaded weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m state_dict\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_folder \u001b[38;5;241m=\u001b[39m save_folder\n",
            "\u001b[0;31mValueError\u001b[0m: Need either a `state_dict` or a `save_folder` containing offloaded weights."
          ]
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883989875
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883989992
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hi, How are You?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
        "inputs"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "{'input_ids': tensor([[    1,  6324, 29892,  1128,   526,   887, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883295969
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(inputs)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "transformers.tokenization_utils_base.BatchEncoding"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883570112
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "tensor = torch.tensor(tokens)\n",
        "\n",
        "outputs = model.generate(tensor)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883703066
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=20)\n",
        "outputs\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:254\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
            "\u001b[0;31mKeyError\u001b[0m: 'shape'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      2\u001b[0m outputs\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:256\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: "
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883384895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/pipelines/__init__.py:868\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    869\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpossible to guess which tokenizer to use. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         )\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
            "\u001b[0;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1691883092245
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}